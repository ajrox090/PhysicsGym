RL (expected 8-10p) [currently 3.3p]

- Introduction (1p)

Reinforcement learning is a field of machine learning that aims at solving goal oriented problems by direct interaction with the environment. An RL framework consists of an agent that learns a task by interacting with a predefined environment. This learning happens by evaluating the rewards for each action taken. The agent then chooses actions in the direction of minimizing the cummulative rewards. There are three important characteristics of RL as highlighted in the book"". i) RL problems are closed loop problems since the action influence future inputs. ii) the agent chooses an action on its own without being explicitly told which action to take and iii) the action taken at time t, also influcences the future state of environments and rewards.

The basic idea behind solving a reinforcement learing problem is that the agent with the help of partial observation is that the agent with the help of partial observation of the environment interacts with the environment by performing an action. This interaction modifies the state of a system and in return gives the agent feedback in the form of rewards. The agent then learns a certain task through this feedback. This interaction-feedback approach is very different from other forms of learning. Supervised and unsupervised learning. In supervised learning the main goal is to infer a model by using a set of labelled training dataset prepared specifically for a problem. Since this model is used to predict future states, the training data must comprise of a set of all diverse possibilities of actions in order to predict unkwon states accurately. For interactive problems, obtaining such a trainng data is not always practical making supervised learning approach implausible for such problems. Reinforcement learning also differs from unsupervised learning which foucsses on finding hidden structures in unlabelled data. Although RL also uses unlabelled data, there is a difference in terms of goal. The goal in RL is to maximize a reward function, however the technique of finding hidden structures in data can enhance the cabailities of RL.

One of the challenges in RL is the tradeoff between exploration and exploitation[.]. In order to maximize the reward, the agent has to consider the actions that proved effective in the past and finding such actions requires exploration in the action space. The idea is to balance between exploration and exploitation so the RL agent is able to learn from past experience while also exploring new sets of actions that yield better rewards.

In recent years, reinforcement learning gained widespread popularity with its ability to learn a task with ease especially in the field of robotics, revenue management and autonomous control of systems. The main focus of this thesis is the application of reinforcement learning for a set of physics problems including but not limited to optimal flow control. For such physics problems, the environment is derived using a set of physics laws expressed in terms of partial differential equations. In the following sections, we will discuss the main components of RL and how to formulate PDE problems as an MDP problems followed by examples.




- Components of RL (0.5-1p)
There are six main components of RL as described in the book[], an environment, an agent, a policy, reward signal, a value function and optionally a model. For the physics problems in focus here, an environment can be considered as the description of the physical world expressed by a single or a combination of partial differential equations. In general an environment consists of state of the system and holds the properties of observability and interactivability. Here observability because the agent must be able to partially observe the environment and the latter because the agent should be able to influence the state of environment with its actions.

The agent is the learner and decision maker. At each timestep, the agent interacts with the environment and the environment updates its state and responses in the form of a scalar value called reward. The goal of the agent is to maximize this reward. This forms the entire specification of a reinforcement learning problem. Following the technical specification as in the book[], consider the agent interacts with the environment for a sequence of timesteps t=1,2,3,.... At each timestep t, the agent receives a partial observation of environment state given by St->S, where S is a set of all possible states. Using St, the agent selects an action At->A(St) where A(St) is the set of all possible actions for state St. This results in a new state St+1 and in return agent receives a scalar reward of form R>R. The act of choosing the actions is done using a mapping from states to probability of each action. A policy determines, the porability of each action for a given state from the action space and selects the ations with highest probability to maximize the cumulative reward. The main learning task for agent is how to update the policy.

Reward in general formalizes the goal of an reinforcement learning agent. The calculation of reward is  done in the environment itself and the agent has no clue of how reward is calculated. As pointed in the book, "Reward is a way to communicate to the agent what needs to be achieved rather than how it should be achieved". For example, in a game of chess, a learning agent receives reward only if its a win, lose or draw and doesn't receive rewards based on how many pieces it won, this is because there are strategies in the game which results in a win even after losing more pieces.

As mentioned previously, the learning happens by maximising a cumulative sum of rewards and not just individual rewards at a timestep. This generally refers to maximising an expected return Gt. For example, for a discrete problem with finite timetep of T, the expected return is given by Gt=Rt+Rt+1+Rt+2,...+RT. where Rt, is the reward at timestep t. this makes sense for discrete tasks but things get a bit more complex for continuous problems. For continuous problems, a terminal state is unknown so equally weighing all the rewards could result in an infinite reward cylce that prevents the agent from exploring new actions. This can be solved by weighing each action differently based on when the rewards are received using a discounting factor called discount rate. The expected return for continous tasks can be formulated as, Gt=Rt+1+VRt+2+.... .Where V is the discount reate and 0<=V<=1. The discount factor is a measure of how important the future rewards are. If V<1, the bounded reward sequence {Rk} results in a finite expected return. For V=0, the agent only considers the immediate reward and as V approaches 1, the agent weighs the future rewards more. The book[] describes the agent as myopic when V=0 and farsighted otherwise. 

Value function is dependent on reward in the sense that rewards describe the quality of action for current timestep where as values represent the quality of actions for all the timesteps. There cannot be a value function without a reward. The environment gives out a reward while values must be estimated separately. The final component as discussed in the book is a model. Model is the learned representation of environment. Basically, predicts how the environment will behave for certain set of actions. This basically helps evaluating the chosen actions before they are actually exectued on the enviornment. The RL methods using a model are termed model-based methods while the others are model-free. Thus thesis focuses solely on model-free methods when its comes to RL.



- MDP (1/2p) + PDE as MDP (1p)
Markov decision process defines the mathematical framework for solving a problem as a reinforcement learning problem. In the book, introduction to reinforcement learning, MDP is deifned as a process that comprises of states that follow Markov property which basically represents those states that retain all relevant information over the time period of environment's execution. We will refer to the formal definiton of MDP as provided in [MDP main paper]: A finite-action discounted MDP is given as M=(X,A,P,R,v); where X and A are state and finite action spaces, P is a mapping from S x A -> R called the transition probability, R is the reward and v is the discount factor.

Next we explain how a pde control problem can be formulated using heat invader example first introduced in paper []. which controls the temperature field measured by a time varying diffuson-convection PDE. Later we will use this example to define components of RL for simple PDE problems using burgers equation, navier stokes equaiton and Kuramoto-Sivashinsky equation. Consider the following convection-diffusion equation:
\Equation
where zzZZZZ represents the domain of enviornment space e.g. room given by its boundary dz that could be the walls of a room. In [], they have considered temperature to be a time-dependent scalar field T:ZXI->R and the velocity field as time-dependent vector field v:ZXI->R2. The time varying source is given by S:ZXI->R. Also, Pe=Lvc/D is the peclet number that depends on the characteristic length of domain and velocity, and the diffusivity constant. V2 is the gradient operator and V calculates the divergence of the laplacian field.

They have used the dirichlet (dz1) boundary condition and neuman boundary (dz2) conditions to solve the pde
\Eq \Eq

This defines a pde control problem of controlling the temperature of room by learning the actions that adjust the boundary temperature Tb(z,t) and velocity (v) of airflow in the room. For a finite action set A with |A|<i, A ={(t,v): a = 1,....}. The state is defined as a vector x=(T,S) and the state space is given by
\EQ
Further, representing pde as a function g of its boundary condiitons applied to a domain, \EQ

Since, this pde is continuous in time, it cannot be applied to MDP framework directly. But, it can be converted to discrete time version by integrating for a time duration of Dt. 
\EQ

Until now, we have defined the observation space and acion space in MDP framework, Next the transition probability kernel. Let X be a random variable that represents distribution of all states, then
\EQ
The reward function, takes state and actions into account and is given by r:XxA->R. For each control problem, the reward changes based on the goal. e.g. In heat invader problem, the goal is to adjust the room temperature to a certain comfort level while balancing the cost of actions like the heating/cooling cost. Considering the comfortable zone of the domain to be Zp<Z, T* the optimal temperature, and the cost function as caction(a), the reward for a particular state-action pair can be defined as,
\EQ

The policy that learns actions is P:X->A. which means tat each timestep we have At=P(xt). Based on this the action-value function can be defined Q:XxA->R 
\EQ
and the optimal value function is given by
\EQ
The RL algorithm uses this optimal value function to choose the best policy that maximizes reward and also minimizes cost.



- PPO (1p)
- Examples (3p)
	1) Navier Stokes equation (Cylinder2DFlowDRL)
	2) Burgers equation (Phiflow)
	3) Heat Equation (heat invader)
	4) Kuramoto Sivashinsky equation








