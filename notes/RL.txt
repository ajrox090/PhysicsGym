RL (expected 8-10p) [currently 3.3p]

- Introduction (1p)

Reinforcement learning is a framework of optimal control and machine learning that aims at solving "goal-oriented" problems by interacting with a complex dynamic system. It basically comprises of an environment and an agent. The environment is a representation of physical system that can be modified e.g. for problems in fluid mechanics, the environment is a function of domain $\Omega \in \mathbf{R}^d$, $E := $\Omega \rightarrow \mathbf{R}^d$* which can be defined using partial differential equations. The agent is a controller that can interact with the environment to control it, keeping an objective in mind following the principles of feedback control. This interaction results into the environment being modified, to which the enivronment responds in the form of a scalar reward as feedback to the actions performed. Learning to control such a system is done by maximizing this reward by taking future state of system into account. The goal of an agent then is to choose actions in the direction of maximizing this cummulative sum of rewards.

This form of learning is different than classical methods of machine learning like supervised learning and unsupervised learning. Supervised learning is about to infer a model by using a set of labelled training dataset prepared specifically for a problem. For better accuracy, the training data must be wholesome, i.e. comprising of enough examples for each type of objective. For many problems, obtaining such a training data is not always possible. Reinforcement learning on the other hand can use partial observations of environment and doesn't require training data in general. Reinforcement learning also differs from unsupervised learning where the main focus is on finding hidden structures in unlabelled data. Even though the information received from the environment is unlabelled, it is always accompanied with some feedback in the form of reward which makes the method of learning in reinforcement learning different from unsupervised learning. The goal in RL is to maximize a reward function, however the technique of finding hidden structures in data can also enhance the cabailities of RL e.g.[].

In recent years, reinforcement learning gained widespread popularity with its ability to learn a task with ease especially in the field of robotics, revenue management and autonomous control of systems. The main focus of this thesis is the application of reinforcement learning for a set of physics problems including but not limited to optimal flow control. For such physics problems, the environment is derived using a set of physics laws expressed in terms of partial differential equations. One of the challenges in RL is the tradeoff between exploration and exploitation[.]. In order to maximize the reward, the agent has to consider the actions that proved effective in the past and finding such actions requires exploration in the action space. The idea is to balance between exploration and exploitation so the RL agent is able to learn from past experience while also exploring new sets of actions that yield better rewards. In the following sections, we will discuss the main components of RL and how to formulate PDE problems as an MDP problems followed by examples.




- Components of RL (0.5-1p)
There are six main components of RL as described in the book[], an environment, an agent, a policy, reward signal, a value function and optionally a model. For the physics problems in focus here, an environment can be considered as the description of the physical world expressed by a single or a combination of partial differential equations and is represented by its state at a certain time. It also holds the properties of observability, the agent is allowed to measure state of the environment partially and interactivability, the agent is allowed to influence the state of environment with its actions.

The agent is the learner and decision maker. At each timestep, the agent interacts with the environment with actions, in return the environment updates its state and responses in the form of a scalar value called reward. The goal of the agent is to maximize this reward. This forms the entire specification of a reinforcement learning problem. Following the technical specification as in the book[], consider the agent interacts with the environment for a sequence of timesteps t=1,2,3,.... At each timestep t, the agent receives a partial observation of environment state given by $S_t \rightarrow S$, where S is a set of all possible states. Using $S_t$, the agent selects an action $A_t \rightarrow A(S_t)$ where $A(S_t)$ is the set of all possible actions for state $S_t$. This results in a new state $S_{t+1}$ and in return agent receives a scalar reward of form $R \in \mathbf{R}$. The policy determines how an agent chooses its action. A policy is a mapping from states to probability of each action. A policy determines, the probaility of each action for a given state from the action space and selects the ations with highest probability to maximize the cumulative reward. The main learning task for agent is how to update the policy.

Reward in general formalizes the goal of a reinforcement learning agent. The calculation of reward is  done in the environment itself and the agent has no clue of how reward is calculated. As pointed in the book, "Reward is a way to communicate to the agent what needs to be achieved rather than how it should be achieved". For example, in a game of chess, a learning agent receives reward only if its a win, lose or draw and doesn't receive rewards based on how many pieces it won, this is because there are strategies in the game which results in a win even after losing more pieces.

As mentioned previously, the learning happens by maximising a cumulative sum of rewards and not just individual rewards at a timestep. This generally refers to maximizing an expected return $G_t$. For example, for a discrete problem with finite timetep of T, the expected return is given by $G_t = R_t+R_{t+1}+R_{t+2},...+R_T$. where $R_t$, is the reward at timestep t. this makes sense for discrete tasks but things get a bit more complex for continuous problems. For continuous problems, a terminal state is unknown so equally weighing all the rewards could result in an infinite reward cylce that prevents the agent from exploring new actions. This can be solved by weighing each action differently based on when the rewards are received using a discounting factor called discount rate. The expected return for continous tasks can be formulated as, $G_t=R_{t+1}+ \Gamma \cdot R_{t+2} + \Gamma^2 R_{t+3} + .... $ where $\Gamma$ is the discount reate and $0 \leq \Gamma \leq 1$. The discount factor is a measure of how important the future rewards are. If $\Gamma$ < 1$, the bounded reward sequence {R_k} results in a finite expected return. For $\Gamma = 0$, the agent only considers the immediate reward and as $\Gamma$ approaches 1, the agent weighs the future rewards more. The agent can be considered myopic when $\Gamma = 0$ and farsighted otherwise[]. 

Just as rewards describe the quality of action for current timestep, a value function represents the quality of actions for all the timesteps. There cannot be a value function without a reward. The environment gives out a reward while values must be estimated separately. A Model is the learned representation of environment and predicts how the environment will behave for certain set of actions. This basically helps evaluating the chosen actions before they are actually exectued on the enviornment. The RL methods using a model are termed model-based methods while the others are model-free. In this thesis we focus solely on model-free methods when its comes to RL.



- MDP (1/2p) + PDE as MDP (1p)
Markov decision process defines the mathematical framework for solving a problem as a reinforcement learning problem. In the book, introduction to reinforcement learning, MDP is deifned as a process that comprises of states that follow Markov property which basically represents those states that retain all relevant information over the time period of environment's execution. We will refer to the formal definiton of MDP as provided in \cite{}: A finite-action discounted MDP is given as $M=(X,A,P,R,\Gamma)$; where X and A are state and action spaces, $P$ is a mapping from $S x A -> \mathbf{R}$ called the transition probability, $R$ is the reward and $\gamma$ is the discount factor.

Next we explain how a pde control problem can be formulated using heat invader example introduced in paper[]. The Heat invader problem is about controlling the temeperature of field measured by a time varying diffusion-convection PDE consisting of heat equation. Consider the following convection-diffusion equation:
\begin{equation*}
\pdv{T}{t} = \nabla \cdot \frac{1}{P_e} \nabla T - \nabla \cdot (vT) + S
\end{equation*}
where $\mathbf{Z} \subset \mathbf{R}^d $ represents the domain of enviornment space e.g. room given by its boundary dz that could be the walls of a room. In [], they have considered temperature to be a time-dependent scalar field $T: Z \times I \rightarrow R$ and the velocity field as time-dependent vector field $v : \mathbf{Z} \times \mathbf{I} \rightarrow \mathbf{R^2}$. The time varying source is given by $ S : \mathbf{Z} \times \mathbf{I} \rightarrow \mathbf{R}$. Also, $Pe = Lv_c/D$ is the peclet number that depends on the characteristic length of domain and velocity, and the diffusivity constant. $\nabla^2$ is the gradient operator and $\nabla$ calculates the divergence of the laplacian field.

They have used the dirichlet (dz1) boundary condition and neuman boundary (dz2) conditions to solve the pde
\Eq \Eq

This defines a pde control problem of controlling the temperature of room by learning the actions that adjust the boundary temperature $T_b(z,t)$ and velocity(v) of airflow in the room. For a finite action set $A$ with $|A| < i$, $A =\{(t,v): a = 1,....\}$. The state is defined as a vector $x = (T, S)$ and the state space is given by
\EQ
Further, representing pde as a function g of its boundary condiitons applied to a domain, \EQ

Since, this pde is continuous in time, it cannot be applied to MDP framework directly. But, it can be converted to discrete time version by integrating for a time duration of $Dt$. 
\EQ

Until now, we have defined the observation space and acion space in MDP framework, Next step is transition probability kernel. Let $X$ be a random variable that represents distribution of all states, then
\EQ
The reward function, takes state and actions into account and is given by $r:X \times A \rightarrow \mathbf{R}$. For each control problem, the reward changes based on the goal. e.g. In heat invader problem, the goal is to adjust the room temperature to a certain comfort level while balancing the cost of actions like the heating/cooling cost. Considering the comfortable zone of the domain to be $Z_p < Z$, $T^*$ the optimal temperature, and the cost function as action(a), the reward for a particular state-action pair can be defined as,
\EQ

The policy that learns actions is $P: X \rightarrow A$. which means tat each timestep we have $A_t=P(x_t)$. Based on this the action-value function can be defined $Q : X \times A \rightarrow R$ 
\EQ
and the optimal value function is given by
\EQ
The RL algorithm uses this optimal value function to choose the best policy that maximizes reward and also minimizes cost.


- Examples (5p)
	1) Navier Stokes equation (1 2/3)
	2) Burgers equation (1 2/3p)
	3) Kuramoto Sivashinsky equation (1 2/3)

		- optimal control problem (2/3)
			- mathematically define what the goal is and formulate the application of control as optimal control problem. i.e. minimizing J
			- is the problem clear? have you explained each part of the problem and reasons behind it at every relevant places?
		- formulate PDE as MDP (2/3)
			- Environment
				- domain specification, inflow profile
				- drag calculation
				- lift calculation
				- reward
			- Control
				- jets (actuation)
		- conclusion (1/3)
	
	- Cylinder2DFlowDRL (https://arxiv.org/pdf/1808.07664.pdf)
	- Deep Reinforcement Learning for Online Control of Stochastic Partial Differential Equations (https://openreview.net/pdf?id=TjECt9pAr4s)
	- Control of chaotic systems by Deep Reinforcement Learning (https://arxiv.org/pdf/1906.07672.pdf)

1) Navier Stokes equation (Cylinder2DFlowDRL)
In the paper "Artificial neural network trained through deep reinforcement learning discover control strategies for active flow control", the authors showcase a very first successful application of reinforcement learning using neural networks to control vortex shredding inside a cylinderical domain. This is the result of solving two problems, drag reduction and wake reduction. Our aim in this section is to analyze how to express a PDE control problems as MDP problems and the experiment in this paper serves as a good groundwork for such an analysis. The experiment consists of 2D simulation of Karman vortex street in a domain of 
Environment:
		- experiment - 2D simulation - Karman vortex street
		- domain - box: L(22 x-axis), H (4.1 y-axis)
		- cylinder - D=1 - vortex shedding
		- Boundary conditions:
			- left wall - inflow - parabolic
			- top & bottom - no slip boundary condition
			- right wall - outflow boundary condition
		- instantaneous drag and lift - formula
		- drag coefficient and lift - formula
		- physics - PDE - Navier-stokes equation

Agent:
	- control - 2 jets - location
	- velocity profile - parabolic
	- properties of jet flow - synthetic jet conditions
	- indirect flow control vs direct flow control
	- apply conditions - low actuation

MDP:
	- steps in RL
	- observation and action space
	- reward
	- policy and value function
	- discount factor

Results:
	- how PPO learns?





































ref: Reinforcement learning for function-valued action space

* Heat Invader- a PDE example:
	- PDE problem
	- define types of action space possible for this example and explain the impact of low-dimensional and high-dimensional control actions on the dimensionality of action space.
	- controlling a pde with boundary conditions
* PDE control as MDP:
	- MDP defintion with what happens at each timestep along with the intuition behind expressing MDP as PDE.
	- explain how to forumlate state space and action spaces. explain dynamics of MDP as that of pde thereby defining the transition probability kernel.
	- introduce how to decide a good reward and discount factor: for heat invader, this reward is based on the comfort level of person in the room and also the cost of actions like energy, electricity.
* Action descriptors and MDP:
	- why are action descriptors needed
	- mdps with action descriptors mathematical explanation
	





























