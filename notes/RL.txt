RL (expected 8-10p) [currently 3.3p]

- Introduction (1p)

Reinforcement learning is a field of machine learning that aims at solving goal oriented problems by interacting with an environment that represents a physical system. The RL framework consists of an agent that learns to control a dynamic system through feedback obtained from interacting with system's environment. This learning requires two things i) the rewards received for each control action taken, called the immediate rewards and ii) a discounted cumulative sum of rewards accounting for future information e.g. a value function. The goal of an agent then is to choose actions in the direction of maximizing this cummulative sum of rewards. This ability to choose actions that influence a system by using feedback of the system, makes reinfrocement learning much suitable for closed-loop problems.

The basic idea behind solving a reinforcement learing problem is that the agent with the help of partial observation is that the agent with the help of partial observation of the environment interacts with the environment by performing an action. This interaction modifies the state of a system and in return gives the agent feedback in the form of rewards. The agent then learns a certain task through this feedback. This interaction-feedback approach is very different from other forms of learning. Supervised and unsupervised learning. In supervised learning the main goal is to infer a model by using a set of labelled training dataset prepared specifically for a problem. Since this model is used to predict future states, the training data must comprise of a set of all diverse possibilities of actions in order to predict unkwon states accurately. For interactive problems, obtaining such a trainng data is not always practical making supervised learning approach implausible for such problems. Reinforcement learning also differs from unsupervised learning which foucsses on finding hidden structures in unlabelled data. Although RL also uses unlabelled data, there is a difference in terms of goal. The goal in RL is to maximize a reward function, however the technique of finding hidden structures in data can enhance the cabailities of RL.

One of the challenges in RL is the tradeoff between exploration and exploitation[.]. In order to maximize the reward, the agent has to consider the actions that proved effective in the past and finding such actions requires exploration in the action space. The idea is to balance between exploration and exploitation so the RL agent is able to learn from past experience while also exploring new sets of actions that yield better rewards.

In recent years, reinforcement learning gained widespread popularity with its ability to learn a task with ease especially in the field of robotics, revenue management and autonomous control of systems. The main focus of this thesis is the application of reinforcement learning for a set of physics problems including but not limited to optimal flow control. For such physics problems, the environment is derived using a set of physics laws expressed in terms of partial differential equations. In the following sections, we will discuss the main components of RL and how to formulate PDE problems as an MDP problems followed by examples.




- Components of RL (0.5-1p)
There are six main components of RL as described in the book[], an environment, an agent, a policy, reward signal, a value function and optionally a model. For the physics problems in focus here, an environment can be considered as the description of the physical world expressed by a single or a combination of partial differential equations. An environment can be perceived as a simulation of a physical system and is represented by its state at a certain time. It also holds the properties of observability, i.e. the agent is able to observe it even if partially and interactivability, i.e. the agent is able to influence the state of environment with its actions.

The agent is the learner and decision maker. At each timestep, the agent interacts with the environment and the environment updates its state and responses in the form of a scalar value called reward. The goal of the agent is to maximize this reward. This forms the entire specification of a reinforcement learning problem. Following the technical specification as in the book[], consider the agent interacts with the environment for a sequence of timesteps t=1,2,3,.... At each timestep t, the agent receives a partial observation of environment state given by St->S, where S is a set of all possible states. Using St, the agent selects an action At->A(St) where A(St) is the set of all possible actions for state St. This results in a new state St+1 and in return agent receives a scalar reward of form R>R. The act of choosing the actions is done using a mapping from states to probability of each action. A policy determines, the porability of each action for a given state from the action space and selects the ations with highest probability to maximize the cumulative reward. The main learning task for agent is how to update the policy.

Reward in general formalizes the goal of an reinforcement learning agent. The calculation of reward is  done in the environment itself and the agent has no clue of how reward is calculated. As pointed in the book, "Reward is a way to communicate to the agent what needs to be achieved rather than how it should be achieved". For example, in a game of chess, a learning agent receives reward only if its a win, lose or draw and doesn't receive rewards based on how many pieces it won, this is because there are strategies in the game which results in a win even after losing more pieces.

As mentioned previously, the learning happens by maximising a cumulative sum of rewards and not just individual rewards at a timestep. This generally refers to maximising an expected return Gt. For example, for a discrete problem with finite timetep of T, the expected return is given by Gt=Rt+Rt+1+Rt+2,...+RT. where Rt, is the reward at timestep t. this makes sense for discrete tasks but things get a bit more complex for continuous problems. For continuous problems, a terminal state is unknown so equally weighing all the rewards could result in an infinite reward cylce that prevents the agent from exploring new actions. This can be solved by weighing each action differently based on when the rewards are received using a discounting factor called discount rate. The expected return for continous tasks can be formulated as, Gt=Rt+1+VRt+2+.... .Where V is the discount reate and 0<=V<=1. The discount factor is a measure of how important the future rewards are. If V<1, the bounded reward sequence {Rk} results in a finite expected return. For V=0, the agent only considers the immediate reward and as V approaches 1, the agent weighs the future rewards more. The book[] describes the agent as myopic when V=0 and farsighted otherwise. 

Value function is dependent on reward in the sense that rewards describe the quality of action for current timestep where as values represent the quality of actions for all the timesteps. There cannot be a value function without a reward. The environment gives out a reward while values must be estimated separately. The final component as discussed in the book is a model. Model is the learned representation of environment. Basically, predicts how the environment will behave for certain set of actions. This basically helps evaluating the chosen actions before they are actually exectued on the enviornment. The RL methods using a model are termed model-based methods while the others are model-free. Thus thesis focuses solely on model-free methods when its comes to RL.



- MDP (1/2p) + PDE as MDP (1p)
Markov decision process defines the mathematical framework for solving a problem as a reinforcement learning problem. In the book, introduction to reinforcement learning, MDP is deifned as a process that comprises of states that follow Markov property which basically represents those states that retain all relevant information over the time period of environment's execution. We will refer to the formal definiton of MDP as provided in [MDP main paper]: A finite-action discounted MDP is given as M=(X,A,P,R,v); where X and A are state and finite action spaces, P is a mapping from S x A -> R called the transition probability, R is the reward and v is the discount factor.

Next we explain how a pde control problem can be formulated using heat invader example first introduced in paper []. which controls the temperature field measured by a time varying diffuson-convection PDE. Later we will use this example to define components of RL for simple PDE problems using burgers equation, navier stokes equaiton and Kuramoto-Sivashinsky equation. Consider the following convection-diffusion equation:
\Equation
where zzZZZZ represents the domain of enviornment space e.g. room given by its boundary dz that could be the walls of a room. In [], they have considered temperature to be a time-dependent scalar field T:ZXI->R and the velocity field as time-dependent vector field v:ZXI->R2. The time varying source is given by S:ZXI->R. Also, Pe=Lvc/D is the peclet number that depends on the characteristic length of domain and velocity, and the diffusivity constant. V2 is the gradient operator and V calculates the divergence of the laplacian field.

They have used the dirichlet (dz1) boundary condition and neuman boundary (dz2) conditions to solve the pde
\Eq \Eq

This defines a pde control problem of controlling the temperature of room by learning the actions that adjust the boundary temperature Tb(z,t) and velocity (v) of airflow in the room. For a finite action set A with |A|<i, A ={(t,v): a = 1,....}. The state is defined as a vector x=(T,S) and the state space is given by
\EQ
Further, representing pde as a function g of its boundary condiitons applied to a domain, \EQ

Since, this pde is continuous in time, it cannot be applied to MDP framework directly. But, it can be converted to discrete time version by integrating for a time duration of Dt. 
\EQ

Until now, we have defined the observation space and acion space in MDP framework, Next the transition probability kernel. Let X be a random variable that represents distribution of all states, then
\EQ
The reward function, takes state and actions into account and is given by r:XxA->R. For each control problem, the reward changes based on the goal. e.g. In heat invader problem, the goal is to adjust the room temperature to a certain comfort level while balancing the cost of actions like the heating/cooling cost. Considering the comfortable zone of the domain to be Zp<Z, T* the optimal temperature, and the cost function as caction(a), the reward for a particular state-action pair can be defined as,
\EQ

The policy that learns actions is P:X->A. which means tat each timestep we have At=P(xt). Based on this the action-value function can be defined Q:XxA->R 
\EQ
and the optimal value function is given by
\EQ
The RL algorithm uses this optimal value function to choose the best policy that maximizes reward and also minimizes cost.


- Examples (5p)
	1) Navier Stokes equation (Cylinder2DFlowDRL) (1 2/3)
		- problem statement (2/3)
		- problem formulation (2/3)
			- domain specification, inflow profile
			- drag caclulation
			- lift calculation
			- jets (actuation)
			- reward 
		- results (1/3)
	2) Burgers equation (1 2/3p)
	(Deep Reinforcement Learning for Online Control of Stochastic Partial Differential Equations
	https://openreview.net/pdf?id=TjECt9pAr4s)
		- problem statement (2/3)
		- problem formulation (2/3)
		- results (1/3)
	4) Kuramoto Sivashinsky equation (1 1/3)
	(Control of chaotic systems by Deep Reinforcement Learning
	https://arxiv.org/pdf/1906.07672.pdf)
		- problem statement (1/3)
		- problem formulation (2/3)
		- results (1/3)

Cylinder2DflowControl
In the paper "Artificial neural network trained through deep reinforcement learning discover control strategies for active flow control", the authors showcase a very first successful application of reinforcement learning using neural networks to control vortex shredding inside a cylinderical domain. This is the result of solving two problems, drag reduction and wake reduction. Our aim in this section is to analyze how to express a PDE control problems as MDP problems and the experiment in this paper serves as a good groundwork for such an analysis. The experiment consists of 2D simulation of Karman vortex street in a domain of 
Environment:
	- 	experiment - 2D simulation - Karman vortex street
	- 	domain - box: L(22 x-axis), H (4.1 y-axis)
	- 	cylinder - D=1 - vortex shedding
	- 	Boundary conditions:
			- left wall - inflow - parabolic
			- top & bottom - no slip boundary condition
			- right wall - outflow boundary condition
	- 	instantaneous drag and lift - formula
	- 	drag coefficient and lift - formula
	- 	physics - PDE - Navier-stokes equation

Agent:
	-	control - 2 jets - location
	- 	velocity profile - parabolic
	- properties of jet flow - synthetic jet conditions
	- indirect flow control vs direct flow control
	- apply conditions - low actuation

MDP:
	- steps in RL
	- observation and action space
	- reward
	- policy and value function
	- discount factor

Results:
	- how PPO learns?





































ref: Reinforcement learning for function-valued action space

* Heat Invader- a PDE example:
	- PDE problem
	- define types of action space possible for this example and explain the impact of low-dimensional and high-dimensional control actions on the dimensionality of action space.
	- controlling a pde with boundary conditions
* PDE control as MDP:
	- MDP defintion with what happens at each timestep along with the intuition behind expressing MDP as PDE.
	- explain how to forumlate state space and action spaces. explain dynamics of MDP as that of pde thereby defining the transition probability kernel.
	- introduce how to decide a good reward and discount factor: for heat invader, this reward is based on the comfort level of person in the room and also the cost of actions like energy, electricity.
* Action descriptors and MDP:
	- why are action descriptors needed
	- mdps with action descriptors mathematical explanation
	





























