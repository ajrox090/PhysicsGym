MPC (5-8p) [currently 3p]

-----------------------------------------------------------------------------------------------------------
* Introduction [1p] (1p)
* Model Predictive control [0.5p] (0.5p)
	- basic idea, explain the general idea of algorithm
	- algorithm
	- some explanationsu

* MPC problems [0p]
	- stabilizing mpc
	- economic mpc

* convection diffusion pde [1.5p] (1.5p)
	- intro
	- boundary condition
	- problem statement
	- solution methods

-----------------------------------------------------------------------------------------------------------


Model predictive control is a powerful optimization strategy for feedback control and has been around since 1980s. It involves using a system model to predict optimal control variables of a dynamic system.
One factor for its popularity is the ability to handle multiple-input multiple-output (MIMO) systems, which the simple controllers like PID fail to optimally control, especially when there are multiple dependent input/output variables. 
Another factor is its ability to incorporate constraints which can involve limiting the domain of control parameters that resembles a real world scenario. For example, consider a control problem of keeping an autonomous car in the desired lane. The plant is the environment in which car is driven e.g. the road with cars in different lanes and the control parameter is the speed of car being controlled. So, a boundary condition could be, the speed of car cannot exceed the speed limit which can be incorporated in MPC by bounding the domain of speed of the car with the speed limit.
Despite these benefits, MPC does require a powerful and fast processor since it solves the optimization problem in an online manner at each timestep. 

figure 

Figure \ref{fig:mpcmain} shows the basic block diagram of a system using MPC as a controller. It involves a system $y(u, t) = f(x,u,t)$ and control input $u(t) = g(x,t)$. MPC controller solves the optimization problem of minimizing a cost function for a certain prediction horizon leading to the prediction of an optimal action for that time-step given by $u$. Once the contorl action is applied it then iteratively calculates the next control action for the new forecasted system state.
This establishes the general idea behind MPC, in the rest of the section we define the components of MPC like cost function along with the optimal control problem and the MPC algorithm as discussed in the thesis[] that uses MPC for partial differential equation control.

\begin{equation*}
    x(k+1) = f(x(k),u(k)) \text{ for } k \in \mathbf{N}
\end{equation*}
where x which represents the state of system at time k and u the control action. Given an initial state $x_0$, iteratively applying $f$ leads to a number of different states known as state trajectories. For example, for control u, the state trajectories are represented by variable x. A control problem for such a system could be to select the control actions that take the system to a desired behavior. Here, the desired behavior could either be to stabilize the system to a predefined state or to prevent the system from entering a certain region[]. In general the desired behavior of a system can be modeled by an optimal control problem which comes from optimal control theory where the main goal is to find a control for a dynamical system such that an objective function is optimized. The objective function (aka. cost function) is basically a mapping from state and action space to a one-dimensional numerical value and is given by J,
\equation a.1
In the above cost function, l should be selected in such a way that it penalizes the deviation from target state. This results in the goal of finding the control sequence uU that minimizes this deviation. Hence, the optimal control problem becomes,
\equation
One more important step to make this control problem more realistic is to apply constraints on the type of values for state and actions. As mentioned earlier, constraints are important for the system to resemble real world behavior. Here, lets say that the set of allowed state and actions are represented by X and U respectively and U is defined as follows,
\equation
The final optimal control problem then becomes,
\equation
Although this is a continuous time problem, MPC aka, receding horizon control, can solve such problems by operating over a finite horizon in time.


MPC generates the trajectory for infinite horizon by solving optimal control problem using the cost function for a finite number of steps N<N and subsequently updating the horizon by selecting only the first action from the resulting control sequences. The finite cost function and the resulting control sequences are given by [],
\equation
\equation
The state of the system is used as input for the optimizer and the optimizer will result in the new control actions, making the state as the feedback and the resulting trajectory of states as the closed-loop trajectory.

The following algorithm showcases these steps formally, as proposed in [].
Algorithm1
For each time instant k = 0, 1, ...
	i) Measure the current state x = x(t) of the system
	ii) Solve the optimal control problem in order to obtain the optimal control sequence
	iii) Apply the first element of un as a control to the system during the next sampling period
	iv) set k = k + 1, and repeat from beginning

In order to take the future costs of actions into account, the optimal value function comes into play which compares the cost at each step with the best cost of each action for an infinite horizon. However, for analysis of stability and performance, optimal value function for a finite horizon is considered in [] and is given by,
\equation




approx 1 page pending
- Talk about stabilizing mpc vs economic mpc?
- talk about different classes of MPC problems? from
https://www.researchgate.net/publication/324748527_DIFFERENT_PROBLEM_CLASSES_AND_SOLUTION_TECHNIQUES_FOR_MODEL_PREDICTIVE_BUILDING_CONTROL







Example
In this section we discuss how to frame a convection diffusion equation control problem as an MPC problem. We discuss one of the two scenarios described in [] of bilinear optimal control problem, where control is applied using boundary control and controlled convection term in a 1D domain.

Convection-diffusion equation [3p]
	- problem statement
	- weak form

	- Solution
		- spatial discretization using Galerkin method
		- time discretization using implicit euler
		- finite dimensional optimal control problem

Convection diffusion equation models the change in physical elements like velocity, temperature in a physical dynamic system in the presence of convective and diffusive processes. Specifically in the problem disscussed here, this equation models the dispersion of temperature inside a room by conductive heat transfer (eg. radiation) and convective transfer induced by velocity field (eg. air flow). Let $\Omega \subset \mathbf{R}^d$, $d \in \{1,2\}$ be a domain, T > 0 and define $Q := (0,T) X \Omega$. The convection diffusion equation is [],
\begin{equation*}
	\begin{split}
		y_t(t,x) - \alpha \Delta y(t,x) + v(t,x) \nabla y(t,x) &= 0   \quad \text{almost everywhere (a.e.) on Q}\\
		y(0,x) = y_0(x) \quad \quad \quad  \text{a.e. in} \Omega\\
	\end{split}
\end{equation*}  
where, $y: Q -> \mathbf{R}$ is the temperature distribution, $\alpha \in \mathbf{R}$ is the diffusion coefficient, $v: [0, T] -> \Omega$ is a velocity field, $y_0: \Omega -> \mathbf{R}$ is the initial temeprature distribution. The equation states that the change in temperature $y_t$ depends on the diffusion term $\alpha \Delta y(t,x)$ and the convective term $v(t,x) \nabla y(t,x)$ in the presence of a velocity field $v(t,x)$

As mentioned in section PDE, in order to solve any pde, the boundary conditions must be specified. In this scenario, the temperature inside the room is influenced by the outside temperature and so the boundary conditions are given by,
\equation
\equation
\equation
\equation
The boundaries conditions can be divided into two parts, TC and Tout, Tc is the one that can be controlled by setting the temperature inside the room by using some form of heating or cooling and Tout is the one that cannot be controlled but is given by the temperature outside the room. Also, for PDEs the two types of boundary conditions, dirichlet and neumann can be inforced by choosing the value of V appropriately. For example, if Vxxxx means setting the temperature at each point in the boundary to some value from function Dc and represents the Dirichlet boundary condition. While, V=0, represents the neumann boundary condition where the temperature at each point on the boundary is 0.

Using the above setting, the problem statement can be defined as follows, Control the temperature inside the room such that it lies within the lower and upper bounds uxx and uxx while minimizing the cost of control actions (e.g. total energy used).
This results in two control actions being performed one at the boundary and the other using the velocity field e.g. a ventilator/fan inside the room. The optimal control problem is then given by[],
\equation
\equation
\equation
\equation
\equation
\equation
\equation
\equation
\equation

Next, we briefly discuss the solution to this minimization problem. For detailed in-depth derivation of terms, we refer to the paper []. In short the approach used in the original paper is called first-discretize-then-optimize approach and involves first calculating the weak form of the equation which involves integrating the equation after multiplying with weighted function to identify those terms in the equation whose derivatives can be balanced. The next step is to discretize in space using Galerkin method which represents the function as linear combination of basis functions. This is equivalent to finite element method discussed in section PDE and involves subdividing the domain O into subsets Ki, e.g. by triangulation. Later polynomial trial functions are applied on each subset[]. This reduces the PDE into nonlinear ODEs.

































