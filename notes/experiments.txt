In this chapter we will compare RL and MPC strategies by experiments for control of Heat equation, Burgers equation and Kuramoto-Sivashinsky equation.

Experiment1: Heat equation control (3p)
    - problem description (<0.5p)
        - fig1a: initial state and target state 
        - fig1b: different initial states (>0, <0)
        - description
    - uncontrolled simulation (<0.5p)
        - fig2a: state transition diagram without control and reference state(0)
        - optimal control
            - fig2b: effect of optimal control sequence on states
    - RL (1 1/3p)
        - fig3: curr state >0, action and reference state at 300 timesteps (6 figures)
        - fig4: curr state <0, action and reference state at 300 timesteps (6 figures)
        - fig5: rl vs random 
    - MPC (1 1/3p)
        - fig6: cont state >0, action and reference state at 300 timesteps (6 figures)
        - fig7: cont state <0, action and reference state at 300 timesteps (6 figures)
        - fig8: predicted actions analysis
    - comparision: RL and MPC (1/3p)

\fig 1a 1b
\fig
\fig
\fig
\fig
\fig
\fig
\fig
\fig
\fig
\fig

A state of environment created by PhysicsGym consists of domain (n) of points and their resolution (dx) that forms the dimension of state vector given by formula, N = n / dx. Here, the state of environment represents the temperature distribution in a room. Taking n=3, dx=0.25, our state vector consists of 12 points. Consider our environment is very diffusive, with a diffusion rate of D = 2.0 then under abritrary initial states, the goal of this experiment is to stabilize the heat equation to 0. Figure 1a shows the state X={x1,x2,...xn} in $BLUE$ initialized uniformly at random and the target state in $RED$. and Figure 1b shows different initial states, including both positive and negative values.*(expand)

\fig 2a 2b
\fig
\fig
\fig
\fig
\fig
\fig
\fig
\fig
\fig
\fig
When the environmet is not influenced by a controller, the heat equation balances itself by reducing the higher values and by increasing the lower values untill an equilibrium is achieved. This means that the heat exchange occurs between colder areas of the room and the hotter areas till the temperature is same everywhere. To stabilize the equation to 0, i.e. bringing the temperature down to 0, we will need to apply external control in the form of forces e.g. using a fan whose airflow is described by a vector field. fig2a shows how the aiflow from fan looks like and the the controller must choose the correct speed of the fan to bring the temperature down to 0 in a fixed number of steps. Here the speed of the fan is in between [-1.0, 1.0] which is used for scaling the magnitude of airflow from fan using the action_transform function as discussed previous chapter.

For the initial state >0 in fig2b, the following negative control sequence achieves our goal, 
An = [-0.4 if i<12 else 0.0 for all actions] and for initial state <0, the positive control sequence works, An=[0.4 if i < 12, else 0.0 for all actions]. Next we will train the RL agent to learn the optimal control sequence by using only the state description as input and the rewards calculated based on how far the state is from reference state.

Next let's discuss the performance of DDPG agent trained for 10 epochs and 200 time steps with the same setup as above.
fig3 shows the actions predicted by the rl agent and the respective effect on change of states for the timesteps, 1,15,30,60,120 and 299. Anaylzing the actions, it is clear that the agent chooses an action of magnitude -1.0 when the state is above 0 (fig3a,3b). When the left half of the state is below 0, the agent reduces this magnitude, still negative (fig3c,3d) and brings the action down to 0 when the equation will balance itself near 0 (fig3e,3f). This shows the agent has learnt the correct magnitude of actions to be performed based on how high the state is above 0. To further verify the generalization, we tested with the negative state, where the state is below 0 so, the optimal actions to be performed should be positive in the beginning and then 0. As shown in the fig4, the agent shows similar performance for negative states by lifting the state above 0 with positive actions and then bringing the actions down to 0 when the equation will balance itself near 0. This shows good generalization of our agent especially because the agent was trained only on positive states. Comparing the performance of RL with random agent and uncontrolled state, in figx we can see that the performance of rl agent is better than random agent.

Next we use MPC to control the heat equation with prediction horizon p=5 and similar timesteps as rl agent. fig6 shows the actions predicted by mpc agent as a result of minimizing the cost function, are similar to rl agent in the beginning of simulation. ...
























// 9 pages approximately.
Inverse Problem: Burgers equation (3p)
- Introduction [PBDL]
    - burgers equation: what it is and its types and how it is used
    - problem statement and goal of experiment
- Interface : 
    - enviornment, physics. How do they apply to this problem
    - and what others things to implement for this experiment.
- Problem definition (Application to inverse problems: PBDL)
    - how to choose observation, action space, transition probability, reward
- Results
    - visualization using lviz and showstate?
- (Documentation: Code Implementation)

Experiment 1:
Burgers equation is one of the simplest PDEs that describes speed of fluid with respect to time and is given by
d/dt u(x,t) = - u(x,t) * d/dx u(x,t) + v * d2/dx2 u(x,t) + F(x,t)
Here, u is the velocity field, v is the viscosity of fluid and F is the optional control force. This viscous burgers equation calculates the change in velocity u(x,t) of a viscous fluid in the presence of an external force. This equation further simplifies to inviscous burgers equation in the presence of inviscous fluid, also used for prototyping equations causing shock-waves in solution.
d/dt u(x,t) + u(x,t) * d/dx u(x,t) = 0

In this experiment we are trying to solve an inverse problem using burgers equation where the goal of rl agent is to learn the tragejectory towards target state. Let us see how our interface is used to frame this problem as an RL problem. There are two important implementations of our interface that we discussed before, the physics interface and the environment interface. The physics interface for burgers equation is inspired from phiflow's interface of Physics object which consists of initialisation of all dependent variables in the constructor e.g. the viscosity and diffusion information. The next imporant method in the physics interface is the step function where the actual PDE calculations are defined. For example, the step function in burgers equation first performs diffusion of velocity field by taking viscosity into account and then it advects the field onto itself using the semi-langrangian method. After that the control forces are applied on the advected field, technically speaking this adds two fields.

The environment implementation is also very simple. The main variables for the environment contructor are the observation spaces and the action space. For this experiment, the observation space consists of information about velocity fields for current state and goal state, and time step information, which is given by the formula x + 2x + 1 where x is the shape of field. Actions are taken in the form of a one dimensional array covering every velocity value which is given by equation prod(x) + len(x). The reset function is called once before the agent starts to learn and it initialises all the variables back to its original state. In this experiment we initialise the velocity with a Gaussian function G=e, and the goal/target state is calculated using the ground truth value of forces defined by another gaussian function that the agent is trying to learn. A reset method always returns an observation of the environment as defined by the observation space.

The final method is the step function that performs three steps. a) convert the actions predicted by agent into the right format that can be processed by physics object. b) update the state by moving the PDE simulation forward in time and by applying forces (actions) c) Calculate the new observation, reward and other relevant information. The reward function here is defined by taking generated forces into account. We use RunningMeanstd by stablebaselines to normalize the reward and the reward is calculated by taking the squared difference of generated forces for each timestep. Additionally, the  distance to the target field, scaled by a predefined factor (FINAL_REWARD_FACTOR) is subtracted at the end of each trajectory.

Results:
Fig 1 shows the step wise transition of states as compared to goal state and ground truth. Here we can see that by the end of last time step, the rl agent is able to reconstruct the trajectory to goal state pretty smooth.

Fig 2 shows the comparision between grount truth, rl agent and uncontrolled simulation. The rl agent here is able to regenerate trajectory pretty well but its not so smooth as the ground truth. As depicted by the blue line, the rl agent is able to capture many curves as simular to ground truth, which is missing in uncontrolled simulation.


Todos:
    - Explain how the results can be improved in terms of choosing the model.
    - Can write about what kinds of problems can be solved with burgers equation.


Heat Equation(0p)
- Introduction []
    - heat equation, what it is and how it is used?
    - problem statement and goal of the experiment
        - trajectory reconstruction problem like above
-Problem definition
    - as defined in heat invader paper.
- Interface
    - environment, physics. wrt. problem
    - other methods needed.
- Results
- (Documentation: Code Implementation)

The heat equation calculates the change in temperature at each point in a domain and is given by nonlinear partial differential equation:
d/dt T(x,t) = -v * d2/dx2 T(x,t) + F(x,t)


Kuramoto Sivashinsky (0p)
- Introduction [chaos paper, ks]
    - what does KS do? what applications?
- Problem definition
    - as defined in chaos paper
- Interface
    - environment, physics wrt. problem
    - other methods needed.
- Results
- (Documentation: Code Implementation)
