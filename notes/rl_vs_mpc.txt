7) RL vs MPC [~2p][1.5p]
    7.0) short introduction?                  
    7.1)                                  (4/3)
        - fig and fundamental difference (terminology) (fig using the example given by professor) (2/3) [1hr]
        - approach difference                   (fig using the example given by professor) (2/3) [0.5hr]
    7.4) computational effort and other differences       (2/3) [1.5hr]


Figure
In this section we discuss some differences in MPC and RL frameworks in terms of terminiology, approach, model usage and computational efforts.
















Above figure shows basic functionality of MPC and RL approaches. There are few similarities on a high level between each component e.g. plant and environment, controller and reward, control input uk and action ak. However there are also some minor differences among them. For example, The state in MPC is related only to the internal properties of the system as defined by the model but in rl this state can also represent both, the internal and external properties of the system. The plant in mpc represents a system process and only gives out system's state information while the environment in MDP also gives out information about reward along with the state. MPC control focusses on minimizing an objective function while MDP is about maximizing the cumulative reward using a value function. In the deterministic setting the immediate reward in MDP, rk+1 can be related to the objective function as follows[]:
rk+1 = - (Jk+1 -Jk)

Technically speaking, mpc approach is different than rl in terms of the feedback received and how the objective function is used. In MPC, the controller solves the optimization problem to choose the best set of actions for a finite amount of time and predicts the future set of actions for a finite amount of time in future using only the state of the system as feedback from the plant. At each timestep k, the obvserver receives a measurement mk from the plant which is converted to the state of the system x that characterizes the controller model at current time[]. Using this information, the controller solves a minimizaiton problem for a finite number of time steps DT. This optimization is done for the objective function J using the controller model F in the presence of constraints for the PDE. Solving this optimization problem results into mapping the state vector to a sequence of actions for a finite horizon out of which only the first action is chosen as the action being applied followed by the system moving forward in time. Thereon, the entire process is repeated. The objective function used in MPC is the integral of a functiin l of the model variables along the prediction horizon[].
\equation

In terms of application of MPC to PDE control, the state vector x typically represnts physical quantities like entire temperature of the room or total velocity of fluid in a domain. u represents the control actions that could be external forces applied to the system e.g. airflow induced by turning a fan ON/OFF. z represents the dependent variables like the indivdual temperature values or velocity at certain point in space at a particular time and d represents the set of uncontrollable variables e.g. heat source or solar irradiation[].

On the other hand, MDP consists of a state sapce X, action space A, transition probability P, reward distribution R and discount factor V. The transition probability function is different from the model F in MPC as it represents the ground truth information rather than being a simplified representation of the system[]. At each timestep, the agent receives a partial observation of the environment in the form of sate vector similar to MPC but here the agent maps the state information to actions using a policy function that internally uses the transition probability function. The output of policy function is an action that is applied to the environment to proceed to the next step and in return the environment responds with a scalar reward Rk describing how good the action was. The main idea behind policy is to maximize a discounted cumulative sum of rewards given by G.
Gk = xxxxxxx


In terms of how the models and function approximations are used, a model in MPC aka. controller model is obtained through techniques like system identification and is a representation of system dynamics. There is also a further requirement of simplifying the controller model to gurantee convergence but this results in performance loss. While in RL, models approximate the policy or value function which does not directly represent system dynamics. Also, the analytical form is not a requirement when using models with RL agent which is useful for highly complex formulations.[]






























