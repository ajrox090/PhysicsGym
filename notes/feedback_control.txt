Feedback Control: (2p) [currently 2 1/3p] 

4) Feedback Control                     (3p)
    4.1) Introduction                   (1.5p)
        - open-loop vs closed-loop
        - optimal control problem
    4.2) Methods for feedback control   (1.5p)
        - conventional methods:		PID
        	- algorithm
        	- advantages
        	- disadvantages
        - MPC for optimal feedback control
        	- algorithm
        	- advantages
        	- disadvantages
        - RL for optimal feedback control
        	- algorithm
        	- advantages disadvantages


In control theory, there are two main types of control system in existence. Openloop and closed-loop. An openloop system involves only one way flow of control, that means the control input is pre-decided based on prior knowledge of system and there is no change in future inputs based on how the system responded to previous inputs. eg. a washing machine takes the time for wash as input control and washes the clothes for that particular time even if the clothes are not washed properly, the washing machine will still stop at the end of cycle. A closed loop on the other hand, measures the output and uses it as feedback to improve the control task. Open loop control can control a system accurately only if the entire model specification of system is known before hand, which is not the case for complex dynamic systems. Hence, the closed-loop strategies which adaptively modify the control behavior are the main go-to strategies for such complex dynamic systems.

Feedback control is a typical closed-loop control system principle that involves influencing the future state behavior of a system using its output in the form of feedback. A feedback control system is composed of controllers, plant/process, sensors, etc. Fig \ref{feedbackcontrol} shows the block diagram of a basic feedback control system. Here, the controller takes as input the desired output response(r) and measurement of previous state $y_m$ and aims to minimize the error between the two. The output of a controller is the control action $u$ that serves as input to the plant in the presence of some distrubances, as in the real world scenario. The plant then updates the state of the system based on the control action $u$ and outputs a new state $y$ which is measured by the sensor and fed back into the controller for the next time step. The whole process is repeated indefinitely or until the system reaches the goal state. For example, consider a control system that adjusts the temperature of a room based on some comfort level using fan as controller. Such a system will take as input, the desired temperature value and the current temperature of the room. The controller will then predict actions that minimize the difference between the two inputs and update the actions based on the feedback from the system.

Many different types of controllers can be formed following the principle of feedback control. One of the most common one is a proportional integral derivative (PID) controller which continuously calculates an error value $e(t)$ as the difference between a desired system response ($r$) and a measured system output ($y_m$) and applies a correction based on proportional, integral, and derivative terms. Fig \ref{pid} shows PID controller in action. 
fig 
The P in PID controller stands for proportional error, $e(t) = K_p \cdot (r(t) - y_m(t))$, where $K_p$ is some constant. Here, more the the deviation from desired state, more the error, hence the name proportional error. If the change is high, the error is going to be higher because of the constant $K_p$. The second term, I integral, takes into account the past error measurements and integrates them over time. For example if there is an error after implementation of proportional control, the integral term can be used to diminish or eliminate this error based on the cumulative values of past error measurements. The final term, D derivative, involves learning the future trend of error values based on how rapidly the error values are changing over a fixed time period. Finally, the output of PID controller is the sum of all the three terms combined together[2],
\begin{equation*}
u(t) = K_p e(t) + K_i \[ \int_{0}^{t} e(\tau) \,d\tau \] + K_d \frac{de(t)}{dt}
\end{equation*}
where the non-negative, $K_p$,$K_i$,$K_d$, denote the coefficients for the proportional, integral, and derivative terms respectively. 

The main advantage of PID controller is that it is simple as it only depends on the measurements of plant and doesn't require any prior knowledge of the system But because of this simplicity, it is not suitable for controlling complex systems that depend of multiple input variables. Apart from this, PID controller doesn't have the ability to deal with constraints[3].

Model predictive control (MPC) is a class of algorithms that aims at learning the model of a system and use it to predict the new state of the system and iteratively improve the model based on the feedback from system. PID controllers are single input and single output (SISO) controllers while controllers in MPC are multiple inputs multiple output controllers. MPC is considered an iterative finite-horizon optimization approach. For a time step of t, MPC controller uses a sampled state to execute a control strategy for a finitie time horizon (t+Dt). For the future time step t+1, the state is sampled again and the calculations are repeated resulting in a new state and control parameters. This is done iteratively until optimal controller performance is achieved based on the defined objective. The prediction horizon keeps being shifted forward and for this reason MPC is also called receding horizon control.

The primary advantage of MPC over PID is its ability to handle constraints and MIMO systems. But it also requires the model of a system which is not always possible for many complex dynamic systems. To overcome this we introduce, a technique of machine learning, reinforcement learning, that shows promising results just based on the data of the system without requiring a complete model of the system.

Reinforcement learning is a class of technique related to machine learning that involves learning from interaction with an environment and has gained widespread popularity because of its promising results in the field of control. The environment in reinforcement learning could be any physical description of real world eg. road for self driving cars, room for cleaning robot, or airfield for a self-driving drone. The learner or agent in RL interacts with environment by performing actions carefully chosen using a learning policy. In return the environment responds with a measure of how good the actions are which is called reward. The general goal of the RL agent is to maximise this reward. It also follows the principle of feedback control as the reward from environment acts as feedback to the controller (agent) which affects the future predictions.


References
[1]Feedback control: https://www.researchgate.net/publication 276831282_How_to_model_and_prove_hybrid_systems_with_KeYmaera_a_tutorial_on_safety
[2]PID (ref: http://d-scholarship.pitt.edu/8171/1/AngLi20100628.pdf)
[3]PID vs MPC https://www.ijareeie.com/upload/2013/november/17_COMPARATIVE.pdf
[4]MPC (ref: https://eng.libretexts.org/Bookshelves/Industrial_and_Systems_Engineering/Book%3A_Chemical_Process_Dynamics_and_Controls_(Woolf)/12%3A_Multiple_Input_Multiple_Output_(MIMO)_Control/12.03%3A_MIMO_using_model_predictive_control#:~:text=handle%20structural%20changes.-,Disadvantages%20of%20MPC,not%20handle%20input%20disturbances%20well.)
