In this section, we compare different research works in solving common physical control problems using RL and MPC methods and motivate the need for our interface as a benchmark. In the first four paragraphs, we talk about papers related to RL and MPC along with their advantages and limitations.  Then we discuss studies which compare these two methods for two different scenarios, adaptive cruise control and controlling the diffusion of heat. The main reason behind this comparison is that both RL and MPC methods have their advantages and disadvantages based on the problem at hand. But there doesn't exist any benchmark that allows for easy implementation and comparison of these methods for partial differential equation problems. This is where our thesis comes in. Later on, we also talk about a similar toolbox named “OpenModelica Mocrogrid Gym” \citep{omg} that does something similar to ours but for microgrid problems.

A 2019 paper \citep{ANN19}, by authors Rabault, Kuchta, Jensen, Reglade and Cerardi, showcases the first application of RL in drag reduction contributing to the discovery of optimal strategies in active flow control. In this paper, the artificial neural network learns an active control strategy from experimenting with the mass flow rates of two jets on the sides of a cylinder resulting in the stabilised vortex and drag reduction by about 8\%. They use FEniCS for simulations and tensorforce \citep{tensorforce} for agent and execution. The environment is first initialised and then executed, where the agent evolves for the given number of iterations by interacting with the environment and obtains reward e.g. in the form of an average drag value for that evolution. They use a class of RL algorithms called “Proximal Policy Optimization” (PPO). The PPO agent further learns to maximize this drag reduction reward over time, which happens internally in tensorforce. Another paper by them, \citep{range20} uses reinforcement learning to discover active flow control strategies using a range of Reynolds numbers (the Reynolds number is defined as the ratio of fluid momentum force to viscous shear force \citep{reynold}). The results show that drag reduction of up to 38\% can be achieved for Reynolds number of 400. It also uses the PPO algorithm with a new smoothing interpolation function which effectively suppresses problematic jumps in the lift. Here we witness the advantage of the RL interface in tensorforce resulting in the minimalistic implementation of the RL framework but there is no easy way to incorporate other RL algorithms within this implementation. 

Another paper \citep{general19} about solving nonlinear differential equations introduces a rule-based self-learning approach using reinforcement learning to find general solutions for nonlinear differential equations like Schrödinger’s equation \citep{schroedinger}, Burgers equation \citep{burger}, Navier Stokes equation \citep{navier} and few more. The reinforcement learning solutions here are more efficient and provide consistent results as compared to discrete physics solutions thereby proving the efficiency of RL in solving certain nonlinear differential equations. Although the work by these authors provides a solid groundwork for RL in flow control, they do not provide portability to other problems e.g. controlling diffusion of heat in a complex system. This requires further abstraction in both RL and FEniCS implementation to bridge this gap.	

The thesis by Simon Pirkelmann \citep{economic_mpc}, shows the usage of model predictive control for time-varying systems in an energy-efficient way. They do so by solving the convection-diffusion equation \citep{cde} within a time-variant system by formulating the problem as a constraint optimization problem. According to the author, learning a closed-loop model appears to be a good choice because the problem is difficult to solve numerically for large or possibly infinite time horizons and with less availability of weather forecast data. They conclude by saying that their results don't prove the convergence of state and control trajectories. Furthermore, they also use FEniCS for solving partial differential equations.