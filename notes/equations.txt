Interface
    - Physics Interface: how a pde is defined
    - Environment Interface: interface between rl and pde
    - Runner Interface: how to run an experiment

// 9 pages approximately.

Inverse Problem: Burgers equation (3p)
- Introduction [PBDL]
    - burgers equation: what it is and its types and how it is used
    - problem statement and goal of experiment
- Interface : 
    - enviornment, physics. How do they apply to this problem
    - and what others things to implement for this experiment.
- Problem definition (Application to inverse problems: PBDL)
    - how to choose observation, action space, transition probability, reward
- Results
    - visualization using lviz and showstate?
- (Documentation: Code Implementation)

Experiment 1:
Burgers equation is one of the simplest PDEs that describes speed of fluid with respect to time and is given by
d/dt u(x,t) = - u(x,t) * d/dx u(x,t) + v * d2/dx2 u(x,t) + F(x,t)
Here, u is the velocity field, v is the viscosity of fluid and F is the optional control force. This viscous burgers equation calculates the change in velocity u(x,t) of a viscous fluid in the presence of an external force. This equation further simplifies to inviscous burgers equation in the presence of inviscous fluid, also used for prototyping equations causing shock-waves in solution.
d/dt u(x,t) + u(x,t) * d/dx u(x,t) = 0

In this experiment we are trying to solve an inverse problem using burgers equation where the goal of rl agent is to learn the tragejectory towards target state. Let us see how our interface is used to frame this problem as an RL problem. There are two important implementations of our interface that we discussed before, the physics interface and the environment interface. The physics interface for burgers equation is inspired from phiflow's interface of Physics object which consists of initialisation of all dependent variables in the constructor e.g. the viscosity and diffusion information. The next imporant method in the physics interface is the step function where the actual PDE calculations are defined. For example, the step function in burgers equation first performs diffusion of velocity field by taking viscosity into account and then it advects the field onto itself using the semi-langrangian method. After that the control forces are applied on the advected field, technically speaking this adds two fields.

The environment implementation is also very simple. The main variables for the environment contructor are the observation spaces and the action space. For this experiment, the observation space consists of information about velocity fields for current state and goal state, and time step information, which is given by the formula x + 2x + 1 where x is the shape of field. Actions are taken in the form of a one dimensional array covering every velocity value which is given by equation prod(x) + len(x). The reset function is called once before the agent starts to learn and it initialises all the variables back to its original state. In this experiment we initialise the velocity with a Gaussian function G=e, and the goal/target state is calculated using the ground truth value of forces defined by another gaussian function that the agent is trying to learn. A reset method always returns an observation of the environment as defined by the observation space.

The final method is the step function that performs three steps. a) convert the actions predicted by agent into the right format that can be processed by physics object. b) update the state by moving the PDE simulation forward in time and by applying forces (actions) c) Calculate the new observation, reward and other relevant information. The reward function here is defined by taking generated forces into account. We use RunningMeanstd by stablebaselines to normalize the reward and the reward is calculated by taking the squared difference of generated forces for each timestep. Additionally, the  distance to the target field, scaled by a predefined factor (FINAL_REWARD_FACTOR) is subtracted at the end of each trajectory.

Results:
Fig 1 shows the step wise transition of states as compared to goal state and ground truth. Here we can see that by the end of last time step, the rl agent is able to reconstruct the trajectory to goal state pretty smooth.

Fig 2 shows the comparision between grount truth, rl agent and uncontrolled simulation. The rl agent here is able to regenerate trajectory pretty well but its not so smooth as the ground truth. As depicted by the blue line, the rl agent is able to capture many curves as simular to ground truth, which is missing in uncontrolled simulation.


Todos:
    - Explain how the results can be improved in terms of choosing the model.
    - Can write about what kinds of problems can be solved with burgers equation.


Heat Equation(0p)
- Introduction []
    - heat equation, what it is and how it is used?
    - problem statement and goal of the experiment
        - trajectory reconstruction problem like above
-Problem definition
    - as defined in heat invader paper.
- Interface
    - environment, physics. wrt. problem
    - other methods needed.
- Results
- (Documentation: Code Implementation)

The heat equation calculates the change in temperature at each point in a domain and is given by nonlinear partial differential equation:
d/dt T(x,t) = -v * d2/dx2 T(x,t) + F(x,t)


Kuramoto Sivashinsky (0p)
- Introduction [chaos paper, ks]
    - what does KS do? what applications?
- Problem definition
    - as defined in chaos paper
- Interface
    - environment, physics wrt. problem
    - other methods needed.
- Results
- (Documentation: Code Implementation)
